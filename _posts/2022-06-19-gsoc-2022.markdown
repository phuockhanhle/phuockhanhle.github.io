---
layout: post
title:  "Google Summer of Code 2022 : DigiKam Image Quality Sorter Algorithms Improvement"
date:   2022-06-19 16:41:53 +0200
categories: jekyll update
---
## Project description
The main idea of IQS in digiKam is to determine the quality of an image and convert it into a score. This score is based on four criteria which are factors sabotaging image: blur, noise, exposure, and compression. The current approach is useful to determine images distorted by one of these reasons. However, the current algorithm presents also some drawbacks : It demands a lots of fine tuning from user's side and it's cannot work on aesthetic image. So, I propose the solution of deep learning algorithm. While the dataset and the paper for aesthetic image quality assessment are free to use, we are capable of constructing a mathematical model that can learn the pattern of a dataset, hence, predict the score of quality. As deep learning is an end-to-end solution, it doesnâ€™t require the setting for the hyperparameters. Hence, we can reduce most of the fine-tuning parts to make this feature easier to use

Check out the [IQS Proposal][project-proposal] for more info about description and algorithm of the project.
## First week 13/06/2022

As described in the proposal, the first week is dedicated for experimenting and reproducing the result of two target algorithm [NIMA][NIMA-repo] and [musiq][musiq-repo].

My purpose is testing and training the deep learning model following the algorithms. Hence, only the code needed for these tasks is extracted from their repos. All is published in my own repose [iqs digikam][iqs-digikam-repo].

These are my principal task in my first week : 
- Install environment for running python code of NIMA and musiq.
- Download dataset of EVA and AVA dataset.
- Write training and testing script for NIMA.
- Adapt the label of 2 datasets for the context of NIMA.

Achievement in first week :
- For now, using python, I can train, evaluate and predict on each image.
- Using the pretrained model of the paper achieves a good performance : MSE = 0.3107 (means the variance is 3.1 on the scale score of IQS digiKam)

**Current problem**:
- Most of images are labeled with score around of 5. This is reasonable as if then annotator can not be sure about the quality of the image, they will give the score of 4, 5 or 6. It makes the problem of evaluation the model. The metric MSE will show that it is a good model even when the model will predict all the image with score of 5. These 2 images shows the distribution of the score on 2 datasets : AVA and EVA:

![AVA analyse](https://github.com/phuockhanhle/phuockhanhle.github.io/blob/main/image/ava_analyse.png?raw=true)

**Figure1** : distribution of score of AVA dataset


![EVA analyse](https://github.com/phuockhanhle/phuockhanhle.github.io/blob/main/image/eva_analyse.png?raw=true)

**Figure2** : distribution of score of EVA dataset


**Ideal to resolve :**
- Augment data of different range / reduce data of concentrated range in order to have balance dataset
- Change metric that consider more weighted on result of different region score


## Second week 20/06/2022

As described in the last post, the current metric for regression (MSE) is not suitable for imbalanced data, hence, I consider a branch new metric :
- As our use-case at the end is classification between 3 class : bad, normal and good image,hence, I split the data into 3 class with same meaning. As most of the images are in class normal, the data is still imbalanced, hence, I use [F1 score][F1_score] for evaluation.
- [Spearman's rank correlation coefficient][SRCC] is a metric to evaluate the similarity between 2 distribution. Hence, the more similar between prediction and reference, the better the performance of the model

After having these 2 metrics, I re-evaluate the function model of NIMA: The checkpoint given by the paper still achieves the best performance : 0.589 on F1 score for class of digiKam

As I explained in last week, we need a balance dataset that the score is evenly distributed. I made research on new dataset for image quality assessment:
- [SPAQ][SPAQ] dataset is a dataset for image quality assessment which evaluates each image by a score from 0 to 100.

For the sake of achieving the performance of the paper without using the checkout proposed by the image, I finetune the model for various situation:
- Fine-tuning on number of epochs : number of training-dense epochs and number of full training model. The best combination is 3 epochs for dense training and 7 epochs for full model training.
- Testing on variant of base model : MobileNet, InceptionV2, InceptionV3, VGG16. The model using VGG16 showed the best precision.
- However, I did not yet archive the performance as the paper.

**Upcoming task:**
- Combining various datasets could create a balanced dataset.
- Training on combined dataset
- Training from the pre-trained weight of the paper.

## Third week 27/06/2022

Although the research wasn't finished yet, we can already use the pretrained model published by NIMA with an acceptable performance. Hence, following the timeline in my proposal, the next step would be using the model in C++.

Before integrating in digiKam's repo, I tried to compile a small C++ with opencv. Then, using module DNN of opencv to read the image, read the pretrained model, and calculate the score of the image. To do that, there are some step to do:
- In Python side, the pretrained model file must be [freeze the weight][freeze-model], this is a specific technique so the model file could be read by opencv
- On C++ side, having opencv from 4.5.5 is a requirement.
- At last, we only need to read model from file, read image, and calculate the output of the model. This is a small example of [inference][inference-c++]

## Fourth and Fifth week 04/07/2022 - 17/07/2022

As mentioned in the second week, I change the metric to consider the problem as classification with 3 classes. However, changing the metric doesn't help us to improve the performance of the model. Because the model still trains with the loss function of regression problem. Secondly, training data is still imbalanced as their score is always around 5. The work of these two weeks is to resolve the problem of imbalanced dataset. I have come to a reasonable solution for digiKam. 

In fact, at the end, we would like to have a label of bad, normal or good for an image. So this is a classification problem. That 's why I realize these steps :
- Re-label the dataset by only 3 class, I separate class by it quality score. 
- In order to have a balance dataset, I use only 9000 images for each class, as most of the images are in normal dataset. 
- Change the last layer of the model to dense of softmax. This is a layer using [softmax][softmax] activation function. Hence, the result is percentage of an image belonged to each class.
- Change the lost function applied to train the model, from earth movers distance to categorical entropy.
- Change also the metric to evaluate the model : the first one is the percentage of true prediction on evaluation set, hence the second one is f1 score on each dataset. While the first matrix is more natural to human's understanding, the second represent the capacity of the model to recognize on each class.

After experimenting, the model shows expected result. To evaluate the model, I use the evaluation set of AVA after labeling with classification. There are *14.702* images with *3.626* images from class 2 ( good images), *6.862* images from class 1 ( normal images), *4.214* images from class 1 ( bad images) The percentage of true prediction is *0.721*. F1 score is *0.764 0.674 0.741* on each class. This [file][AVA-pred-file] shows the label and the prediction of each image of AVA dataset.

This is the result after training and testing with different base model and hyperparameter. I conclude to this configuration : 
- Base model : [InceptionResNetV2][InceptionResNetV2]
- Batch size : 16
- Learning rate : 0.0005
- Number epochs : 8
- dropout rate : 0.75

**Main current problem** : As the purpose is to have a balanced data, I choose the threshold is 5 and 6, it means if the score <= 5, the image is labeled as 0, 5 < score <= 6 is for normal, and the rest is for good image. This is too much objectif, so the images that are labeled 0 is not really bad, or the images are labeled 2 is not really too well, it gives the confusion for model.

**Solution** Using combined dataset could be a good solution. 

## Sixth week 18/07/2022 - 24/07/2025

As I would like to spend more time on research after the first evaluation, I changed a little in my plan. In this week, I realize the main classes of aesthetic detection and intergrate to digiKam. As I have changed the architechture of the model in the last week, the aesthetic deteciton cli shoule be changed also. The main problem is to perform exactly the preprocess image from python to c++.

Before implementing the core class of aesthetic detection, I did a small unittest. I get 3 images which are classified as good image in AVA dataset, and 3 images of bad quality that are already in data of image quality sort.

![Good-image-for-unit-test](https://github.com/phuockhanhle/phuockhanhle.github.io/blob/main/image/aesthetic_1.jpg?raw=true)

**Figure3:** Aesthetic image from AVA dataset

![Bad image for unit test](https://github.com/phuockhanhle/phuockhanhle.github.io/blob/main/image/general_bad_image_1.jpg?raw=true)

**Figure4:** Bad image from IQS digiKam unit test

After having the unit test, I implement the aesthetic detector in order to pass the unit test. Fortunately, the architechture of detector of image quality sorter is well defined. Hence, I implement only the main functions of aesthetic detection based on the aesthetic cli. There are only 3 main methods :
- *Preprocessing* : The preprocessing process should be exactly the same to what is done in python. The image is transformed from blue-green-red mode to red-green-blue mode, then resized to 22x224 with INTER_NEAREST_EXACT interpolation. At the end, the eah pixel is normalize to -1 and 1.
- *Serving model* : The model is loaded from a pb file. This is the model file of tensorflow. Opencv dnn is developed well in this path.
- *Postprocessing*: The output of a deep learning model is only a matrix with score. Based on the meaning of the last layer, these scores have different meaning. In our case, the score is a matrix of 3 float number representing the probability that the image is belonged to each class.

The implementation of aesthetic detection passes all unit test. However, we have the problem of the similarity between aesthetic cli and aesthetic detector. While they gives both same prediction of the class, the score is slightly different. The reason is the way thay read the image from file. While the cli uses *imread* method from opencv to read the image, the detector uses loading image thread of digikam with size of 1024 x 1024. This could be cause the problem in the future.

**Current problem** : The file model path should be dynamic to the repo of digikam. However, the file is too large and can not be handled by github. For now, I hardcode the link of the model file. The model file can be downloaded from [here][Model-path]

## First phase resume

This part is dedicated to resume the achivements, problems, TODO list and idea in the first phase of GSOC22.
Achievements:
- (*research side*) Complete data pipeline for AVA and EVA dataset.
- (*research side*) Analyze data to retrive the main problem : imbalanced dataset -> most of image are labeled with score from 4 to 6 in scale of 10.
- (*research side*) Train, test and, experiment [NIMA][NIMA-repo] on AVA and EVA dataset -> confirm the problem of imbalanced dataset.
- (*research side*) Change the problem to 3-classes classification based on digikam context, re-implement data labeling and change the last layer of the model to adapt the new context. Achieve an acceptable result on AVA dataset evaluation set :  72.1% of true prediction.
- (*integration side*) Implement aesthetic detection cli that can recieve a model path and a image path, to get the score of quality of the image.
- (*integration side*) Implement aesthetic detector classes in digikam code base and aesthetic unit test.

Problem:
- (*research side*) Labeling class image based on its score is too much objectif. Hence, the model is easy to confuse between bad image and normal image, or normal image and good image.
- (*research side*) MUSISQ is not yet well researched as it is implemented in non-popular framework JAX, and the pretrained file can not be read by opencv.
- (*integration side*) The position of model file should be managed dynamically by the repo.

TODO:
- (*research side*) Concatenate dataset and train on this dataset -> more generalized dataset better model 
- (*research side*) Reseach on Musiq.
- (*integration side*) Impelement UI of aesthetic detection and management of model file.


[project-proposal]: https://summerofcode.withgoogle.com/media/user/3bea17365af2/proposal/znmmTvwbY9aBIkA7.pdf
[iqs-digikam-repo]: https://github.com/phuockhanhle/iqs-digikam
[NIMA-repo]: https://github.com/idealo/image-quality-assessment
[musiq-repo]: https://github.com/google-research/google-research/tree/master/musiq
[SRCC]: https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient
[F1_score]: https://medium.com/synthesio-engineering/precision-accuracy-and-f1-score-for-multi-label-classification-34ac6bdfb404
[SPAQ]: https://github.com/h4nwei/SPAQ
[freeze-model]: https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc
[inference-c++]: https://github.com/phuockhanhle/iqs-digikam/blob/inference_cpp/inference/inference_iqs/inference_iqs.cpp
[softmax]: https://en.wikipedia.org/wiki/Softmax_function
[InceptionResNetV2]: https://keras.io/api/applications/inceptionresnetv2/
[Model-path]: https://drive.google.com/file/d/1c4uVuyLp_eqE1vLCEVgU0LXv1ik5YgA1/view?usp=sharing
[AVA-pred-file]: https://raw.githubusercontent.com/phuockhanhle/iqs-digikam/main/data/AVA_dataset/NIMA_config/pred.json